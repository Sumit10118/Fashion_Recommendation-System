{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd347df-e9be-48bb-b658-599be8cdef16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44441, 2064)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the merged dataset (you can replace this with the path to your file)\n",
    "merged_data_df = pd.read_pickle('final_data.pkl')  # Or use read_csv if needed\n",
    "\n",
    "merged_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd64dc2d-42ec-40fd-b102-6ebdc1b6315b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Shirts', 'Jeans', 'Watches', 'Track Pants', 'Tshirts', 'Socks',\n",
       "       'Casual Shoes', 'Belts', 'Flip Flops', 'Handbags', 'Tops', 'Bra',\n",
       "       'Sandals', 'Shoe Accessories', 'Sweatshirts', 'Deodorant',\n",
       "       'Formal Shoes', 'Bracelet', 'Lipstick', 'Flats', 'Kurtas',\n",
       "       'Waistcoat', 'Sports Shoes', 'Shorts', 'Briefs', 'Sarees',\n",
       "       'Perfume and Body Mist', 'Heels', 'Sunglasses', 'Innerwear Vests',\n",
       "       'Pendant', 'Nail Polish', 'Laptop Bag', 'Scarves', 'Rain Jacket',\n",
       "       'Dresses', 'Night suits', 'Skirts', 'Wallets', 'Blazers', 'Ring',\n",
       "       'Kurta Sets', 'Clutches', 'Shrug', 'Backpacks', 'Caps', 'Trousers',\n",
       "       'Earrings', 'Camisoles', 'Boxers', 'Jewellery Set', 'Dupatta',\n",
       "       'Capris', 'Lip Gloss', 'Bath Robe', 'Mufflers', 'Tunics',\n",
       "       'Jackets', 'Trunk', 'Lounge Pants', 'Face Wash and Cleanser',\n",
       "       'Necklace and Chains', 'Duffel Bag', 'Sports Sandals',\n",
       "       'Foundation and Primer', 'Sweaters', 'Free Gifts', 'Trolley Bag',\n",
       "       'Tracksuits', 'Swimwear', 'Shoe Laces', 'Fragrance Gift Set',\n",
       "       'Bangle', 'Nightdress', 'Ties', 'Baby Dolls', 'Leggings',\n",
       "       'Highlighter and Blush', 'Travel Accessory', 'Kurtis',\n",
       "       'Mobile Pouch', 'Messenger Bag', 'Lip Care', 'Face Moisturisers',\n",
       "       'Compact', 'Eye Cream', 'Accessory Gift Set', 'Beauty Accessory',\n",
       "       'Jumpsuit', 'Kajal and Eyeliner', 'Water Bottle', 'Suspenders',\n",
       "       'Lip Liner', 'Robe', 'Salwar and Dupatta', 'Patiala', 'Stockings',\n",
       "       'Eyeshadow', 'Headband', 'Tights', 'Nail Essentials', 'Churidar',\n",
       "       'Lounge Tshirts', 'Face Scrub and Exfoliator', 'Lounge Shorts',\n",
       "       'Gloves', 'Mask and Peel', 'Wristbands', 'Tablet Sleeve',\n",
       "       'Ties and Cufflinks', 'Footballs', 'Stoles', 'Shapewear',\n",
       "       'Nehru Jackets', 'Salwar', 'Cufflinks', 'Jeggings', 'Hair Colour',\n",
       "       'Concealer', 'Rompers', 'Body Lotion', 'Sunscreen', 'Booties',\n",
       "       'Waist Pouch', 'Hair Accessory', 'Rucksacks', 'Basketballs',\n",
       "       'Lehenga Choli', 'Clothing Set', 'Mascara', 'Toner',\n",
       "       'Cushion Covers', 'Key chain', 'Makeup Remover', 'Lip Plumper',\n",
       "       'Umbrellas', 'Face Serum and Gel', 'Hat', 'Mens Grooming Kit',\n",
       "       'Rain Trousers', 'Body Wash and Scrub', 'Suits', 'Ipad', nan],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data_df['articleType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf6773c-e237-4935-95d8-b3c527fdca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  articleType  gender__Unisex  gender__Women  season_Spring  \\\n",
      "0  15970.0       Shirts             0.0            0.0            0.0   \n",
      "1  39386.0        Jeans             0.0            0.0            0.0   \n",
      "2  59263.0      Watches             0.0            1.0            0.0   \n",
      "3  21379.0  Track Pants             0.0            0.0            0.0   \n",
      "4  53759.0      Tshirts             0.0            0.0            0.0   \n",
      "\n",
      "   season_Summer  season_Winter  season_na  usage__Ethnic  usage__Formal  ...  \\\n",
      "0            0.0            0.0        0.0            0.0            0.0  ...   \n",
      "1            1.0            0.0        0.0            0.0            0.0  ...   \n",
      "2            0.0            1.0        0.0            0.0            0.0  ...   \n",
      "3            0.0            0.0        0.0            0.0            0.0  ...   \n",
      "4            1.0            0.0        0.0            0.0            0.0  ...   \n",
      "\n",
      "   feature_2039  feature_2040  feature_2041  feature_2042  feature_2043  \\\n",
      "0      0.004744      0.105801      0.000000      0.000000      0.000833   \n",
      "1      0.000000      0.049441      0.000000      0.000000      0.000000   \n",
      "2      0.012989      0.000000      0.084675      0.010639      0.000000   \n",
      "3      0.034288      0.008307      0.063428      0.027466      0.016652   \n",
      "4      0.049037      0.016746      0.061395      0.015871      0.004185   \n",
      "\n",
      "   feature_2044  feature_2045  feature_2046  feature_2047        Label  \n",
      "0      0.076037      0.000000      0.012249      0.004576         Tops  \n",
      "1      0.017049      0.008319      0.019859      0.000000      Bottoms  \n",
      "2      0.015109      0.005990      0.005688      0.033060  Accessories  \n",
      "3      0.037235      0.002308      0.017937      0.038164      Bottoms  \n",
      "4      0.014783      0.000000      0.029446      0.018527         Tops  \n",
      "\n",
      "[5 rows x 2065 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with 44,444 rows already loaded\n",
    "\n",
    "# Define the label mappings based on your previous categorization\n",
    "label_mappings = {\n",
    "    'Shirts': 'Tops',\n",
    "    'Tshirts': 'Tops',\n",
    "    'Sweatshirts': 'Tops',\n",
    "    'Tunics': 'Tops',\n",
    "    'Tops': 'Tops',\n",
    "    'Jeans': 'Bottoms',\n",
    "    'Trousers': 'Bottoms',\n",
    "    'Shorts': 'Bottoms',\n",
    "    'Jeggings': 'Bottoms',\n",
    "    'Capris': 'Bottoms',\n",
    "    'Lounge Pants': 'Bottoms',\n",
    "    'Track Pants': 'Bottoms',\n",
    "    'Casual Shoes': 'Footwear',\n",
    "    'Sandals': 'Footwear',\n",
    "    'Flip Flops': 'Footwear',\n",
    "    'Flats': 'Footwear',\n",
    "    'Heels': 'Footwear',\n",
    "    'Sports Shoes': 'Footwear',\n",
    "    'Booties': 'Footwear',\n",
    "    'Watches': 'Accessories',\n",
    "    'Belts': 'Accessories',\n",
    "    'Handbags': 'Accessories',\n",
    "    'Backpacks': 'Accessories',\n",
    "    'Caps': 'Accessories',\n",
    "    'Sunglasses': 'Accessories',\n",
    "    'Scarves': 'Accessories',\n",
    "    'Jewellery Set': 'Accessories',\n",
    "    'Bracelet': 'Accessories',\n",
    "    'Ring': 'Accessories',\n",
    "    'Necklace and Chains': 'Accessories',\n",
    "    'Earrings': 'Accessories',\n",
    "    'Pendant': 'Accessories',\n",
    "    'Bra': 'Innerwear',\n",
    "    'Innerwear Vests': 'Innerwear',\n",
    "    'Night suits': 'Sleepwear',\n",
    "    'Nightdress': 'Sleepwear',\n",
    "    'Baby Dolls': 'Sleepwear',\n",
    "    'Churidar': 'Ethnic Wear',\n",
    "    'Salwar and Dupatta': 'Ethnic Wear',\n",
    "    'Salwar': 'Ethnic Wear',\n",
    "    'Kurtas': 'Ethnic Wear',\n",
    "    'Kurtis': 'Ethnic Wear',\n",
    "    'Lehenga Choli': 'Ethnic Wear',\n",
    "    'Patiala': 'Ethnic Wear',\n",
    "    'Sarees': 'Ethnic Wear',\n",
    "    'Jackets': 'Seasonal Clothing',\n",
    "    'Rain Jacket': 'Seasonal Clothing',\n",
    "    'Lounge Tshirts': 'Seasonal Clothing',\n",
    "    'Ties': 'Seasonal Clothing',\n",
    "    'Ties and Cufflinks': 'Seasonal Clothing',\n",
    "    'Swimwear': 'Seasonal Clothing',\n",
    "    'Deodorant': 'Personal Care',\n",
    "    'Lipstick': 'Personal Care',\n",
    "    'Foundation and Primer': 'Personal Care',\n",
    "    'Sunscreen': 'Personal Care',\n",
    "    'Body Lotion': 'Personal Care',\n",
    "    'Face Wash and Cleanser': 'Personal Care',\n",
    "    'Makeup Remover': 'Personal Care',\n",
    "    'Face Serum and Gel': 'Personal Care'\n",
    "}\n",
    "\n",
    "# Function to map articleType to the corresponding label\n",
    "def apply_label(article):\n",
    "    return label_mappings.get(article, 'Other')  # Default to 'Other' if not found\n",
    "\n",
    "# Apply the function to create a new column for labels\n",
    "merged_data_df['Label'] = merged_data_df['articleType'].apply(apply_label)\n",
    "\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(merged_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e9e7b2-773d-4047-9a7c-fa2d8387a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Track total time for process\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Create and save candidate pairs based on attributes\n",
    "def generate_candidate_pairs(df):\n",
    "    start_time = time.time()\n",
    "    tops = df[df['articleType'] == 'Tops']\n",
    "    bottoms = df[df['articleType'].isin(['Jeans', 'Trousers', 'Shorts'])]\n",
    "\n",
    "    candidate_pairs = []\n",
    "    for _, top_row in tops.iterrows():\n",
    "        for _, bottom_row in bottoms.iterrows():\n",
    "            if is_matching_attributes(top_row, bottom_row):  # Define attribute matching logic here\n",
    "                candidate_pairs.append((top_row['id'], bottom_row['id']))\n",
    "\n",
    "    candidate_pairs_df = pd.DataFrame(candidate_pairs, columns=['Top_ID', 'Bottom_ID'])\n",
    "    candidate_pairs_df.to_pickle(\"candidate_pairs.pkl\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Total candidate pairs generated: {len(candidate_pairs_df)}\")\n",
    "    print(f\"Time taken to generate candidate pairs: {end_time - start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c3d722-760b-427f-9639-91064538ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if attributes match\n",
    "def is_matching_attributes(top_row, bottom_row):\n",
    "    # Define your matching logic here for attributes like gender, season, usage\n",
    "    matching_gender = (top_row['gender__Unisex'] == bottom_row['gender__Unisex'] or \n",
    "                       top_row['gender__Women'] == bottom_row['gender__Women'])\n",
    "    matching_season = (top_row['season_Spring'] == bottom_row['season_Spring'] or\n",
    "                       top_row['season_Summer'] == bottom_row['season_Summer'] or\n",
    "                       top_row['season_Winter'] == bottom_row['season_Winter'] or\n",
    "                       top_row['season_na'] == bottom_row['season_na'])\n",
    "    matching_usage = (top_row['usage__Ethnic'] == bottom_row['usage__Ethnic'] or\n",
    "                      top_row['usage__Formal'] == bottom_row['usage__Formal'] or\n",
    "                      top_row['usage__Other'] == bottom_row['usage__Other'] or\n",
    "                      top_row['usage__Sports'] == bottom_row['usage__Sports'])\n",
    "    return matching_gender and matching_season and matching_usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0b4b1f-4ae5-493b-8b26-7573565908c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for the entire process: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Track total time for process\n",
    "total_start_time = time.time()\n",
    "# Run the pairing and matching process\n",
    "#generate_candidate_pairs(merged_data_df)\n",
    "#process_matching_pairs(merged_data_df)\n",
    "\n",
    "total_end_time = time.time()\n",
    "print(f\"Total time taken for the entire process: {total_end_time - total_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec5d3a6-7ab7-478d-9546-40defde50aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process candidate pairs with cosine similarity and save matches\n",
    "\n",
    "def process_matching_pairs(df, batch_size=500):\n",
    "    candidate_pairs_df = pd.read_pickle(\"candidate_pairs.pkl\")\n",
    "    num_batches = int(np.ceil(len(candidate_pairs_df) / batch_size))\n",
    "    \n",
    "    similarity_start_time = time.time()\n",
    "    \n",
    "    for batch_num in range(num_batches):\n",
    "        batch_start_time = time.time()\n",
    "        batch_pairs = candidate_pairs_df.iloc[batch_num * batch_size : (batch_num + 1) * batch_size]\n",
    "        matching_results = []\n",
    "\n",
    "        for _, (top_id, bottom_id) in batch_pairs.iterrows():\n",
    "            top_row = df[df['id'] == top_id].iloc[0]\n",
    "            bottom_row = df[df['id'] == bottom_id].iloc[0]\n",
    "\n",
    "            # Calculate cosine similarity for features\n",
    "            top_features = top_row[[f'feature_{i}' for i in range(2048)]].values\n",
    "            bottom_features = bottom_row[[f'feature_{i}' for i in range(2048)]].values\n",
    "            image_similarity = cosine_similarity([top_features], [bottom_features])[0][0]\n",
    "\n",
    "            # Define a threshold for cosine similarity to determine a match\n",
    "            if image_similarity >= 0.8:\n",
    "                matching_results.append({'Top_ID': top_id, 'Bottom_ID': bottom_id, 'Is_Matching': True})\n",
    "\n",
    "        # Save each batch of matches\n",
    "        with open(f'matching_results_batch_{batch_num + 1}.pkl', 'wb') as f:\n",
    "            pickle.dump(matching_results, f)\n",
    "\n",
    "        batch_end_time = time.time()\n",
    "        print(f\"Batch {batch_num + 1} of {num_batches} completed in {batch_end_time - batch_start_time:.2f} seconds and saved.\")\n",
    "\n",
    "    similarity_end_time = time.time()\n",
    "    print(f\"Total time taken for cosine similarity calculations: {similarity_end_time - similarity_start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ccdc59-3899-45cc-bbd2-a4781c65bf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2961927, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_data_df = pd.read_pickle('candidate_pairs.pkl')  # Or use read_csv if needed\n",
    "\n",
    "\n",
    "candidate_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a7146f-58cd-4c24-b7a9-45cb5bb52298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Top_ID     Bottom_ID\n",
      "count  2.961927e+06  2.961927e+06\n",
      "mean   2.874044e+04  2.730874e+04\n",
      "std    1.628784e+04  1.506340e+04\n",
      "min    1.978000e+03  1.567000e+03\n",
      "25%    1.276900e+04  1.335100e+04\n",
      "50%    3.095600e+04  2.700600e+04\n",
      "75%    4.177800e+04  3.980700e+04\n",
      "max    5.989800e+04  5.981400e+04\n"
     ]
    }
   ],
   "source": [
    "print(candidate_data_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c9d9091-59e1-4d14-9e44-f7e88b8269cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top_ID</th>\n",
       "      <th>Bottom_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42419.0</td>\n",
       "      <td>39386.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42419.0</td>\n",
       "      <td>18005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42419.0</td>\n",
       "      <td>54924.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42419.0</td>\n",
       "      <td>26994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42419.0</td>\n",
       "      <td>10257.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Top_ID  Bottom_ID\n",
       "0  42419.0    39386.0\n",
       "1  42419.0    18005.0\n",
       "2  42419.0    54924.0\n",
       "3  42419.0    26994.0\n",
       "4  42419.0    10257.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a7ea25c-b2e8-4ba7-93b7-a50f03e0132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs generated: Positive: 96, Negative: 904\n",
      "Pairs generated: Positive: 200, Negative: 1800\n",
      "Pairs generated: Positive: 307, Negative: 2693\n",
      "Pairs generated: Positive: 422, Negative: 3578\n",
      "Pairs generated: Positive: 514, Negative: 4486\n",
      "Pairs generated: Positive: 612, Negative: 5388\n",
      "Pairs generated: Positive: 712, Negative: 6288\n",
      "Pairs generated: Positive: 820, Negative: 7180\n",
      "Pairs generated: Positive: 917, Negative: 8083\n",
      "Pairs generated: Positive: 1005, Negative: 8995\n",
      "Pairs generated: Positive: 1110, Negative: 9890\n",
      "Pairs generated: Positive: 1208, Negative: 10792\n",
      "Pairs generated: Positive: 1325, Negative: 11675\n",
      "Pairs generated: Positive: 1441, Negative: 12559\n",
      "Pairs generated: Positive: 1539, Negative: 13461\n",
      "Pairs generated: Positive: 1662, Negative: 14338\n",
      "Pairs generated: Positive: 1768, Negative: 15232\n",
      "Pairs generated: Positive: 1867, Negative: 16133\n",
      "Pairs generated: Positive: 1989, Negative: 17011\n",
      "Pairs generated: Positive: 2103, Negative: 17897\n",
      "Pairs generated: Positive: 2224, Negative: 18776\n",
      "Pairs generated: Positive: 2332, Negative: 19668\n",
      "Pairs generated: Positive: 2435, Negative: 20565\n",
      "Pairs generated: Positive: 2541, Negative: 21459\n",
      "Pairs generated: Positive: 2650, Negative: 22350\n",
      "Pairs generated: Positive: 2743, Negative: 23257\n",
      "Pairs generated: Positive: 2827, Negative: 24173\n",
      "Pairs generated: Positive: 3000, Negative: 25000\n",
      "Pairs generated: Positive: 4000, Negative: 25000\n",
      "Pairs generated: Positive: 5000, Negative: 25000\n",
      "Pairs generated: Positive: 6000, Negative: 25000\n",
      "Pairs generated: Positive: 7000, Negative: 25000\n",
      "Pairs generated: Positive: 8000, Negative: 25000\n",
      "Pairs generated: Positive: 9000, Negative: 25000\n",
      "Pairs generated: Positive: 10000, Negative: 25000\n",
      "Pairs generated: Positive: 11000, Negative: 25000\n",
      "Pairs generated: Positive: 12000, Negative: 25000\n",
      "Pairs generated: Positive: 13000, Negative: 25000\n",
      "Pairs generated: Positive: 14000, Negative: 25000\n",
      "Pairs generated: Positive: 15000, Negative: 25000\n",
      "Pairs generated: Positive: 16000, Negative: 25000\n",
      "Pairs generated: Positive: 17000, Negative: 25000\n",
      "Pairs generated: Positive: 18000, Negative: 25000\n",
      "Pairs generated: Positive: 19000, Negative: 25000\n",
      "Pairs generated: Positive: 20000, Negative: 25000\n",
      "Pairs generated: Positive: 21000, Negative: 25000\n",
      "Pairs generated: Positive: 22000, Negative: 25000\n",
      "Pairs generated: Positive: 23000, Negative: 25000\n",
      "Pairs generated: Positive: 24000, Negative: 25000\n",
      "Pairs generated: Positive: 25000, Negative: 25000\n",
      "Total pairs generated: 50000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming merged_data_df and candidate_data_df are already defined\n",
    "similarity_threshold = 0.7  # Adjust as needed\n",
    "results = []\n",
    "\n",
    "# Define the number of pairs you want to generate\n",
    "target_pairs_per_label = 25000  # Total target pairs will be 50,000\n",
    "\n",
    "# Get feature vectors in a numpy array\n",
    "features = merged_data_df[[f'feature_{i}' for i in range(2048)]].values\n",
    "\n",
    "# Use counters to track the number of pairs generated\n",
    "positive_count = 0\n",
    "negative_count = 0\n",
    "\n",
    "# Unique IDs\n",
    "top_ids = candidate_data_df['Top_ID'].unique()\n",
    "bottom_ids = candidate_data_df['Bottom_ID'].unique()\n",
    "\n",
    "# Generate pairs until target is reached\n",
    "np.random.seed(42)  # For reproducibility\n",
    "while positive_count < target_pairs_per_label or negative_count < target_pairs_per_label:\n",
    "    random_top_id = np.random.choice(top_ids)\n",
    "    random_bottom_id = np.random.choice(bottom_ids)\n",
    "\n",
    "    # Get the feature vectors for the randomly selected IDs\n",
    "    top_features = features[merged_data_df['id'] == random_top_id]\n",
    "    bottom_features = features[merged_data_df['id'] == random_bottom_id]\n",
    "\n",
    "    if top_features.size > 0 and bottom_features.size > 0:\n",
    "        similarity = cosine_similarity(top_features, bottom_features)[0][0]\n",
    "        \n",
    "        # Classify based on similarity threshold\n",
    "        if positive_count < target_pairs_per_label and similarity >= similarity_threshold:\n",
    "            label = 1\n",
    "            positive_count += 1\n",
    "        elif negative_count < target_pairs_per_label and similarity < similarity_threshold:\n",
    "            label = 0\n",
    "            negative_count += 1\n",
    "        else:\n",
    "            continue  # Skip this iteration if no labels can be assigned\n",
    "        \n",
    "        # Store the pair with its label\n",
    "        results.append((random_top_id, random_bottom_id, label))\n",
    "        \n",
    "        # Print progress after every 1000 pairs\n",
    "        if (positive_count + negative_count) % 1000 == 0:\n",
    "            print(f\"Pairs generated: Positive: {positive_count}, Negative: {negative_count}\")\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "siamese_pairs_df = pd.DataFrame(results, columns=['Top_ID', 'Bottom_ID', 'Label'])\n",
    "\n",
    "# Save the DataFrame to a pickle file\n",
    "siamese_pairs_df.to_pickle(\"siamese_pairs3.pkl\")\n",
    "\n",
    "# Print final summary\n",
    "print(f\"Total pairs generated: {len(siamese_pairs_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd081c52-2592-450c-a3f5-c47ca5eccfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_pairs_df = pd.read_pickle('siamese_pairs3.pkl')  # Or use read_csv if needed\n",
    "\n",
    "# Display the first few rows to verify the loaded data\n",
    "siamese_pairs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c7063d2-b372-4016-89c2-74acbbc4e4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "Label\n",
      "0    25000\n",
      "1    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_distribution = siamese_pairs_df['Label'].value_counts()\n",
    "print(f\"Label Distribution:\\n{label_distribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db0c9a81-107b-4d5e-8cc3-29942cd2fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "top_ids = siamese_pairs_df['Top_ID'].values\n",
    "bottom_ids = siamese_pairs_df['Bottom_ID'].values\n",
    "labels = siamese_pairs_df['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbea9907-3e60-488d-80ea-336c9737b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Siamese model\n",
    "def create_siamese_model(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "\n",
    "    # Shared layers\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18cbac02-2e33-447d-b0c5-62658ab8a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base network\n",
    "input_shape = (2048,)  # Change this if your features have a different shape\n",
    "base_network = create_siamese_model(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d716c22c-ca69-4a62-b87c-3af46f7f9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for the two branches\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "590ca8b7-4db1-4c27-bb3b-df86f5f146fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the encoded features for both inputs\n",
    "encoded_a = base_network(input_a)\n",
    "encoded_b = base_network(input_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d608700-96a8-44cd-aa48-c11dbf420e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute difference\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "286da9bc-5f55-48cb-8f4b-d58fa254d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amitk\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a custom distance function\n",
    "def euclidean_distance(vects):\n",
    "    return K.sqrt(K.sum(K.square(vects[0] - vects[1]), axis=1, keepdims=True))\n",
    "\n",
    "# Calculate distance\n",
    "distance = Lambda(euclidean_distance)([encoded_a, encoded_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65d75e70-c3ef-405c-8b8e-768a4b3ecd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "siamese_model = Model(inputs=[input_a, input_b], outputs=distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "673ccdef-9593-4e92-b795-a4a5332d38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "siamese_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e0cf587-31a3-40d3-aeb9-51b7f4e03aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ functional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">270,528</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│                               │                           │                 │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
       "│                               │                           │                 │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ functional (\u001b[38;5;33mFunctional\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │         \u001b[38;5;34m270,528\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│                               │                           │                 │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
       "│                               │                           │                 │ functional[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">270,528</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m270,528\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">270,528</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m270,528\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the model summary\n",
    "siamese_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc47878a-376f-41c0-9962-7941b9066d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# Change the learning rate\n",
    "learning_rate = 0.001  # Try a smaller learning rate\n",
    "adam = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23f8a630-11b5-4b02-a865-e487f6c61890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training pairs and labels\n",
    "pairs = np.array(list(zip(top_ids, bottom_ids)))  # Pair up top and bottom features\n",
    "labels = labels  # Labels corresponding to these pairs (already extracted)\n",
    "\n",
    "# Split into training and validation sets (e.g., 80% train, 20% val)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pairs_train, pairs_val, y_train, y_val = train_test_split(pairs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare input data for training\n",
    "X_train_a = np.array([pair[0] for pair in pairs_train])  # First elements of pairs\n",
    "X_train_b = np.array([pair[1] for pair in pairs_train])  # Second elements of pairs\n",
    "\n",
    "# Prepare input data for validation\n",
    "X_val_a = np.array([pair[0] for pair in pairs_val])  # First elements of validation pairs\n",
    "X_val_b = np.array([pair[1] for pair in pairs_val])  # Second elements of validation pairs\n",
    "\n",
    "class_weight ={0: 1., 1: 5.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f4dca-4d23-4833-b3ba-4ad8cfda93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = siamese_model.fit(\n",
    "    [X_train_a, X_train_b],  # Input for the model\n",
    "    y_train,                 # Labels\n",
    "    validation_data=([X_val_a, X_val_b], y_val),  # Validation data\n",
    "    epochs=10,               # Set the number of epochs\n",
    "    batch_size=32,            # Choose a batch size\n",
    "    class_weight=class_weight\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "33b64156-e453-490e-b323-8f0ea8392ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4983 - loss: 1.2764\n",
      "Validation Loss: 1.2597678899765015\n",
      "Validation Accuracy: 0.5055999755859375\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      1.00      0.67      5056\n",
      "           1       0.00      0.00      0.00      4944\n",
      "\n",
      "    accuracy                           0.51     10000\n",
      "   macro avg       0.25      0.50      0.34     10000\n",
      "weighted avg       0.26      0.51      0.34     10000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[5056    0]\n",
      " [4944    0]]\n",
      "ROC AUC Score: 0.24397864468118882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitk\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amitk\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amitk\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on validation data\n",
    "val_loss, val_accuracy = siamese_model.evaluate([X_val_a, X_val_b], y_val)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Generate predictions for further analysis (e.g., classification report, confusion matrix)\n",
    "y_pred = siamese_model.predict([X_val_a, X_val_b])\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_val, y_pred_binary))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, y_pred_binary)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_val, y_pred)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
